---
title: "Cours_21"
author: "Simon Gabay"
date: "`r Sys.Date()`"
output:
  html_notebook:
    number_sections: yes
    toc: yes
    toc_float: yes
---

```{r, setup, fig.show=hold, fig.margin=TRUE}
if(!require("knitr")){
  install.packages("knitr")
  library(knitr)
}
knitr::opts_chunk$set(echo = TRUE, fig.width=20)
```

## Préparer la session de travail

```{r}
setwd("~/GitHub/Cours_2020_stylo/Cours08")
```

```{r, warning=FALSE, results='hide'}
if (!require("udpipe")) install.packages("udpipe")
library(udpipe)

if (!require("filesstrings")) install.packages("filesstrings")
library(filesstrings)

if (!require("textreuse")) install.packages("textreuse")
library(textreuse)
```

# 1. Fonctionnement

```{r}
exemple <- "S'ils furent ma blessure, ils seront mon Achille, S'ils furent mon venin, le scorpion utile"
tokenize_words(exemple)
```

```{r}
tokenize_ngrams(exemple, n = 3)
```

```{r}
tokenize_skip_ngrams(exemple, n = 3, k = 2)
```

avec cette technique, il est possible de comparer deux textes et d'obtenir un score. Ce score est obtenu avec la [similarité de Jaccard](https://fr.wikipedia.org/wiki/Indice_et_distance_de_Jaccard), utilisée en statistiques pour comparer la similarité et la diversité entre des échantillons.

Comparons deux extraits de pièces de théâtre: Du Ryer et Pradon

```{r}
#Pierre Du Ryer, Alcimédon
temoin_1 <-  tokenize_words(paste("Que ferai-je Philante en l'état où je suis ?",
                                 "Ha pour mourir plutôt que n'ai-je plus d'ennuis !",
                                  "Il me semble déjà que je vois la cruelle"))
#Pradon, Phèdre et Hippolyte
temoin_2 <-  tokenize_words(paste("J’aperçois Phèdre, ah ! cachons notre flamme, ",
                                 "Et craignons que nos yeux ne trahissent notre âme.",
                                  "Je ne réponds de rien en l’état où je suis."))
jaccard_similarity(temoin_1, temoin_2)
```

On peut analyser la différence plutôt que la similarité

```{r}
jaccard_dissimilarity(temoin_1, temoin_2)
#est l'équivalent de
1-jaccard_similarity(temoin_1, temoin_2)
```

# 2. Préparation du texte

## Udpipe

Il va falloire télécharger le package `UDpipe`. Pour le français du XVIIème s. nous conseillons `french-gsd`, même s'il est (très) loin d'être parfait.

```{r, warning=FALSE, results='hide'}
if (!require("udpipe")) install.packages("udpipe")
library(udpipe)
```

## Préparation de la lemmatisation

Nous allons lemamtiser le texte à l'aide dun modèle `UDPipe`. Il est en effet plus facile de travailler sur des lemmes pour diminuer la taille du lexique.

Si vous n'avez pas de modèles (il y en a un dans le dossier), vous pouvez les télécharger à cet endroit.

If you do not have an udpipe model, you can download one here (and adjust _infra_ the name)
```{r}
#m<-udpipe_download_model(language =  "french-gsd")
#m<-udpipe_download_model(language =  "french-partut")
#m<-udpipe_download_model(language =  "french-sequoia")
```

On charge le texte

```{r}
file_name<- "corpus_1_reg_txt_clean/Campistron_Achille_reg.txt"
#Il pesut être utile de créer un dossier spécifique, pour mettre toutes les images produites dans cet endroit
folder_name <-"corpus_1_reg_txt_clean/"
```

Nous pouvons désormais charger le texte, et le modèle.

```{r}
udpipe_model_french <- udpipe_load_model(file = "tools/french-gsd-ud-2.4-190531.udpipe")
text_to_lemmatise <- readLines(file_name)
#variable pour les sauvegardes
#nom de fichier sans extension
file_name_noExt <- tools::file_path_sans_ext(file_name)
#ajout d'une extention
filename <- paste(file_name_noExt,"_LEM.txt", sep="")
```

# Lemmatisation

```{r}
#on transforme le texte en une longue chaîne de caractères séparés par des espaces.
text_to_lemmatise <- paste(text_to_lemmatise, collapse = " ")
# on lemmatise cette chaîne
x <- udpipe_annotate(udpipe_model_french, x = text_to_lemmatise)
#je fais un data.frame
x <- as.data.frame(x)
# voir le résultat
View(x)
```

on a une colonne

```{r}
data.frame(x$token,x$lemma)
```
Je lemmatise tout

```{r}
#je donne le nom du dossier à traiter
dir<-"corpus_1_reg_txt_clean"
#je liste les fichiers à l'intérieur
folder<-list.files(path=dir, full.names = TRUE)
#ºe lance une boucle pour traiter les fichies les uns après les autres
for (file in folder){
  #j'importe le doc
  text_to_lemmatise <- readLines(file)
  #je le transforme en un seul vecteur
  text_to_lemmatise <- paste(text_to_lemmatise, collapse = " ")
  #je sauvegarde le nom du fichier sans l'extenstion
  file_name_noExt <- tools::file_path_sans_ext(file)
  #je rajoute `_LEM.txt` à la fin du fichier sans extension pour le différencier
  filename <- paste(file_name_noExt,"_LEM.txt", sep="")
  #Je lemmatise le texte
  x <- udpipe_annotate(udpipe_model_french, x = text_to_lemmatise)
  #je sauvegarde le résultat dans un data frame
  x <- as.data.frame(x)
  #je sauvegarde le premier lemme dans le fichier `_LEM.txt`
  cat(x$lemma[1], "", file = paste(filename))
  #je boucle sur la colonne `x$lemma` et je les ajoute les uns après les autres dans `_LEM.txt`
  for(i in 2:length(x$token_id)){
    if(x$sentence_id[i] != x$sentence_id[i-1])
    cat("\n", file = filename, append = T)
    if(is.na(x$lemma[i]))
    next
    cat(x$lemma[i], "", file = filename, append = T)
  }
}
```

```{r, warning=FALSE, results='hide'}
if (!require("filesstrings")) install.packages("filesstrings")
library(filesstrings)
```

```{r}
#je crée un nouveau dossier
dir.create("corpus_1_reg_txt_LEM")
#je fais la liste des fichiers que je viens de créer
folder<-list.files(path=dir, pattern="*LEM.txt", full.names = TRUE)
#je les déplace un à un dans le nouveau dossier
for(file in folder){
  file.move(file,"corpus_1_reg_txt_LEM")
}
```

# 3. Comparer les textes

## Télécharger et charger le package textreuse

```{r, warning=FALSE, results='hide'}
if (!require("textreuse")) install.packages("textreuse")
library(textreuse)
```

```{r}
corpus = TextReuseCorpus(dir = "corpus_1_reg_txt_LEM/", tokenizer = tokenize_ngrams, n = 3)
```

```{r}
corpus
#on donne le noms des pièces
names(corpus)
#on donne le nombre de tokens
wordcount(corpus)
```

Il est possible de comparer les textes à grande échelle

```{r}
#je reprends la similarité de jaccard, que je calcule pour toutes les paires de mon dossier
comps = pairwise_compare(corpus, jaccard_similarity)
#Je transforme le résultat en un data-frame facile à visualiser
comps_df <- as.data.frame(comps)
#J'affiche le dataframe
View(comps_df)
```

Autre mode d'affichage: toutes les paires à la suite avec leur score de similarité

```{r}
paires<-pairwise_candidates(comps)#[order(pairwise_candidates(comps)[, 3], decreasing = TRUE), ]
View(paires)
```

Je prends les deux pièces ayant une similarité élevée, et je regarde où sont les similitudes

```{r}
#Je charge mon premier témoin
Statira = TextReuseTextDocument(file = "corpus_1_reg_txt_LEM/Pradon_Statira_reg_LEM.txt", tokenizer = tokenize_ngrams, n = 3)
#Je charge mon deuxième témoin
Scipion = TextReuseTextDocument(file = "corpus_1_reg_txt_LEM/Pradon_Scipion_reg_LEM.txt", tokenizer = tokenize_ngrams, n = 3)
#J'aligne les n-grams
alignement<- align_local(Statira, Scipion)
#J'affiche le résultat
alignement
```

```{r}
#Je charge mon premier témoin
Statira = TextReuseTextDocument(file = "corpus_1_reg_txt_LEM/Pradon_Statira_reg_LEM.txt", tokenizer = tokenize_ngrams, n = 3)
#Je charge mon deuxième témoin
Scipion = TextReuseTextDocument(file = "corpus_1_reg_txt_LEM/Pradon_Scipion_reg_LEM.txt", tokenizer = tokenize_ngrams, n = 3)
#J'aligne les n-grams
alignement<- align_local(Statira, Scipion)
#J'affiche le résultat
alignement
```


```{r}
Andromaque = TextReuseTextDocument(file = "corpus_1_reg_txt_LEM/Racine_Andromaque_reg_LEM.txt", tokenizer = tokenize_ngrams, n = 3)
alignement<- align_local(Statira, Andromaque)
alignement
```

# Approche Minhash

## Explications

On nomme fonction de hachage, de l'anglais _hash function_, une fonction particulière qui, à partir d'une donnée fournie en entrée, calcule une empreinte numérique servant à identifier rapidement la donnée initiale. (cf.[Wikipedia](https://fr.wikipedia.org/wiki/Fonction_de_hachage)). Pour faire simple et vite: on transforme le texte en une série de clefs numériques. On parle de _Locality sensitive hashing_ (cf. [Wikipedia](https://fr.wikipedia.org/wiki/Locality_sensitive_hashing)).

```{r}
#Je prends un exmple
exemple <- paste("Que ferai-je Philante en l'état où je suis ?",
                                 "Ha pour mourir plutôt que n'ai-je plus d'ennuis !",
                                  "Il me semble déjà que je vois la cruelle")
#Je pl'importe en faisant des n-gram de 3 tokens
exemple_tr <- TextReuseTextDocument(exemple, meta = list(id = "my_id"),
                                    tokenizer = tokenize_ngrams, n = 3,
                                    keep_tokens = TRUE) 
#j'affiche le résultat
tokens(exemple_tr)
```

Je peux faire en sorte de transformer chacun de ces n-gram en hashes. L'intérêt est de transformer des chaînes de caractères en chiffres, plus aisément exploitables, mais surtout de diminuer considérablement la taille de la mémoire nécessaire (et donc de gagner en vitesse de calcul).

```{r}
#j'affiche le même résultat sous la forme de hashes
hashes(exemple_tr)
#Pour y voir plus clair, je peux afficher tout cela sous la forme d'un data frame
merge(tokens(exemple_tr),hashes(exemple_tr))
```

Je peux développer ce système, en utilisant des minhashes: je convertis une liste de tokens en un nombre _n_ (_infra_ 240) de tokens sélectionnés au hasard et transformés en hashes eux-mêmes.

```{r}
#reprenons notre exemple, que l'on découpe en trigrammes
exemple_3<- tokenize_ngrams(exemple, n = 3)
exemple_3
minhash <- minhash_generator(n = 240)
exemple_3_MinH <- minhash(exemple_3)
exemple_3_MinH
```

Je peux transformer ces hases en minhashes
```{r}
test<-minhashes(exemple_tr)
test
```


Pour comparer ce type de données, on a recours à un équivalent de la similarité de Jaccard, le _locality-sensitive hashing algorithm_ (fonction `lsh()`). En découpant le hash, j'obtiens un équivalent de la similarité de Jaccard.

```{r}
#h= nombre de shingles sélectionnés aléatoirement
lsh_threshold(h = 240, b = 80)
lsh_threshold(h = 240, b = 120)
lsh_threshold(h = 240, b = 240)
```

# En pratique

Je peux donc répéter l'opération à plus grande échelle

```{r, warning=FALSE}
minhash <- minhash_generator(n = 240)
corpus <- TextReuseCorpus(dir = "corpus_1_reg_txt_LEM/",
                          tokenizer = tokenize_ngrams, n = 3,
                          minhash_func = minhash,
                          keep_tokens = TRUE,
                          progress = FALSE)
```

On retrouve donc notre corpus

```{r}
corpus[[1]]
```

Que l'on peut afficher sous la forme de hashes

```{r}
head(minhashes(corpus[[1]]))
```

Je vais créer des "bouquets" (_buckets_) grâce au _Locality-sensitive hashing_. En gros: j'attribue des valeurs de hachage proches à des n-grammes (plus préciséement des _shingles_, c'est à dire des n-grammes de mots) qui sont proches.

J'ai ici 2,400 rangs, car 240 hashes pour chacun des dix textes. Nous utilisons 240 car nos textes sont très différents, et un nombre plus faible (par exp. 120) ne serait pas efficace pour observer de petites nuances

```{r}
buckets = lsh(corpus, bands = 240, progress = FALSE)
buckets
```

On (re-)convertit mes données en texte

```{r}
candidates <- lsh_candidates(buckets)
candidates
```

Et on compare chacune de ces paires en fonction de la similarité de Jaccard

```{r}
#Je compare
resultat<-lsh_compare(candidates, corpus, jaccard_similarity, progress = FALSE)
#Je transforme le résultat en data frame pour rendre le document aisément lisible
resultat_df<-as.data.frame(resultat)
#Je regarde le résultat
View(resultat_df)
```